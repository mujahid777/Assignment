{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b28c02-856b-448a-92b5-ae6ffdb9b2fd",
   "metadata": {},
   "source": [
    "## Q1. What is the KNN algorithm?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric method, meaning it does not make assumptions about the underlying data distribution.\n",
    "\n",
    "In KNN, the training dataset consists of labeled examples with known class or target values. The algorithm uses these examples to make predictions for new, unseen data points. The main idea behind KNN is that similar data points are likely to have similar class labels or target values.\n",
    "\n",
    "Here's how the KNN algorithm works:\n",
    "\n",
    "1.Data Preparation: First, you need to have a labeled training dataset, where each data point has known class labels or target values. \n",
    "  Additionally, you need to decide the value of the hyperparameter K, which represents the number of nearest neighbors to consider for making predictions.\n",
    "\n",
    "2.Distance Calculation: KNN uses a distance metric (e.g., Euclidean distance, Manhattan distance) to measure the similarity between data points.\n",
    "  For each new data point to be classified or predicted, the distances to all other data points in the training set are computed.\n",
    "\n",
    "3.K Nearest Neighbors: The K nearest neighbors are selected based on the smallest distances to the new data point. \n",
    "  These neighbors form the neighborhood around the new data point.\n",
    "\n",
    "4.Majority Voting (Classification): In classification tasks, KNN counts the class labels of the K nearest neighbors and assigns the most frequent\n",
    "   class label as the predicted class for the new data point. This is known as majority voting.\n",
    "\n",
    "5.Weighted Voting (Regression): In regression tasks, KNN assigns a weighted average of the target values of the K nearest neighbors as\n",
    "  the predicted value for the new data point. The weights can be based on the inverse of the distances or other factors.\n",
    "\n",
    "6.Prediction: After determining the class label or target value for the new data point, the prediction is made.\n",
    "\n",
    "The choice of K is crucial in KNN. A smaller K value can lead to more flexible decision boundaries and can be sensitive to noisy data, while a larger K value can lead to smoother decision boundaries but might overlook local patterns. The optimal K value is often determined through experimentation or cross-validation.\n",
    "\n",
    "KNN is a simple yet effective algorithm, especially for smaller datasets. It doesn't require training or building a model as it directly uses the training data for predictions. However, it can be computationally expensive when the dataset is large, as it requires calculating distances to all training points for each prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae8186-5278-410f-a1e7-2724a0321383",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing the value of K in KNN is an important step as it can significantly impact the performance of the algorithm. The selection of K depends on the characteristics of the dataset and the trade-off between bias and variance. Here are a few common approaches to choose the value of K in KNN:\n",
    "\n",
    "\n",
    "    1.Cross-Validation: Cross-validation is a popular technique to evaluate the performance of a model on a dataset. You can use it to choose the optimal value of K in KNN. The basic idea is to split your training data into multiple folds, train the KNN model on different values of K, and evaluate the performance on the validation set. By comparing the performance across different K values, you can select the one that provides the best balance between bias and variance.\n",
    "\n",
    "\n",
    "    2.Rule of Thumb: A commonly used rule of thumb is to set K as the square root of the number of data points in the training set. For example, if you have 100 training samples, you can start with K=10 (square root of 100). This rule provides a rough estimate but might need adjustment based on the specific dataset and problem.\n",
    "\n",
    "\n",
    "    3.Domain Knowledge: Your domain knowledge and understanding of the problem can guide you in selecting an appropriate K value. For instance, if you know that the classes in your dataset are well-separated, choosing a smaller K may help capture local patterns. On the other hand, if the classes have overlapping boundaries, a larger K might be better to capture the overall trends.  \n",
    "\n",
    "    4.Experimentation: Sometimes, it is necessary to try out different values of K and observe the performance of the algorithm. You can start with a small range of K values, evaluate the model's performance on a validation set, and select the one that achieves the best results. Visualizing the decision boundaries for different K values can also provide insights into the behavior of the algorithm.\n",
    "\n",
    "It's important to note that the optimal value of K may vary depending on the dataset and the specific problem you are working on. Therefore, it is recommended to try multiple approaches, evaluate the performance, and choose the value of K that yields the best results for your specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe844a-bc6c-4845-a969-c2295679e33b",
   "metadata": {},
   "source": [
    "##  Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "\n",
    "The difference between the KNN classifier and KNN regressor lies in the nature of the prediction task they perform:\n",
    "\n",
    "\n",
    "    1.KNN Classifier: The KNN classifier is used for classification tasks, where the goal is to assign a categorical label or class to a given input data point. It determines the class membership based on the majority vote of the K nearest neighbors. The predicted class is assigned to the new data point based on the most frequent class label among the K neighbors. For example, if the majority of the K nearest neighbors belong to class A, the KNN classifier will predict class A for the new data point.\n",
    "\n",
    "\n",
    "    2.KNN Regressor: The KNN regressor, on the other hand, is used for regression tasks, where the goal is to predict a continuous or numeric target value for a given input data point. Instead of selecting the majority vote, the KNN regressor calculates the weighted average of the target values of the K nearest neighbors. The weights can be based on the inverse of the distances or other factors. The predicted value is then assigned to the new data point as the regression output. For example, if the K nearest neighbors have target values of 10, 12, 15, 16, and 18, the KNN regressor may predict a value of 14.2 for the new data point.\n",
    "\n",
    "In summary, the key distinction between the KNN classifier and KNN regressor lies in the type of output they produce. The classifier assigns categorical labels or classes, while the regressor predicts continuous or numeric values. The choice between using the KNN classifier or KNN regressor depends on the nature of the problem and the type of the target variable you are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498ba08-d189-47ff-8709-9871ea1fd10d",
   "metadata": {},
   "source": [
    "##  Q4. How do you measure the performance of KNN?\n",
    "\n",
    "To measure the performance of the K-Nearest Neighbors (KNN) algorithm, various evaluation metrics can be used. The choice of evaluation metric depends on whether you are performing classification or regression tasks. Here are some commonly used performance metrics for KNN:\n",
    "\n",
    "For Classification Tasks:\n",
    "\n",
    "\n",
    "    1.Accuracy: Accuracy is a widely used metric for classification tasks. It calculates the percentage of correctly classified instances out of the total instances in the dataset. It provides an overall measure of the model's correctness.\n",
    "\n",
    "\n",
    "    2.Precision and Recall: Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. Recall (also known as sensitivity or true positive rate) measures the proportion of correctly predicted positive instances out of all actual positive instances. These metrics are useful when dealing with imbalanced datasets or when the cost of false positives or false negatives is different.\n",
    "\n",
    "\n",
    "    3.F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance, particularly when you want to consider both precision and recall simultaneously.\n",
    "\n",
    "\n",
    "    4.Confusion Matrix: A confusion matrix provides a detailed breakdown of the model's performance, showing the number of true positives, true negatives, false positives, and false negatives. It can be used to derive various evaluation metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "For Regression Tasks:\n",
    "    \n",
    "\n",
    "\n",
    "    1.Mean Absolute Error (MAE): MAE measures the average absolute difference between the predicted values and the true values. It provides a straightforward measure of the model's accuracy.\n",
    "\n",
    "\n",
    "    2.Mean Squared Error (MSE): MSE calculates the average squared difference between the predicted values and the true values. It amplifies the impact of larger errors compared to MAE.\n",
    "\n",
    "\n",
    "    3.Root Mean Squared Error (RMSE): RMSE is the square root of MSE and provides an interpretable metric in the same unit as the target variable. It is widely used and useful for comparing different models.\n",
    "\n",
    "\n",
    "    4.R-squared (R2) Score: R2 score represents the proportion of the variance in the target variable that is explained by the model. It ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates no improvement over the mean.\n",
    "\n",
    "To evaluate the performance of KNN, you can calculate one or more of these metrics using appropriate validation techniques such as cross-validation or hold-out validation. It is important to choose evaluation metrics that align with the specific requirements and characteristics of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8c991-5634-4366-b496-71a180708a82",
   "metadata": {},
   "source": [
    "##  Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The \"curse of dimensionality\" refers to a set of challenges and issues that arise when working with high-dimensional data in machine learning algorithms, including the K-Nearest Neighbors (KNN) algorithm. It describes the phenomenon where the performance and effectiveness of many algorithms deteriorate as the number of dimensions (features) in the dataset increases.\n",
    "\n",
    "The curse of dimensionality can affect KNN in several ways:\n",
    "\n",
    "\n",
    "    1.Increased Sparsity: As the number of dimensions increases, the data becomes increasingly sparse in the feature space. In high-dimensional spaces, data points tend to become farther apart from each other. This sparsity can lead to a loss of local structure, making it more difficult for KNN to identify meaningful patterns and find nearest neighbors accurately.\n",
    "\n",
    "\n",
    "    2.Increased Computational Complexity: The computational cost of KNN increases exponentially with the number of dimensions. As the dimensionality grows, the number of possible combinations and distances that need to be computed also increases rapidly. This results in significantly higher computational requirements and longer processing times, making KNN less efficient for high-dimensional data.\n",
    "\n",
    "\n",
    "    3.Degraded Discriminative Power: In high-dimensional spaces, the relative distances between data points become less informative. All data points tend to be equidistant or nearly equidistant from each other, making it harder to identify relevant neighbors for classification or regression tasks. This diminishes the discriminative power of KNN as the effectiveness of distance-based measures decreases.\n",
    "\n",
    "\n",
    "    4.Increased Risk of Overfitting: With high-dimensional data, KNN is more prone to overfitting. Due to the increased number of dimensions, the algorithm can capture noise or irrelevant features, leading to poor generalization performance on unseen data. This is known as the \"Hughes phenomenon,\" where the training data becomes less representative of the overall data distribution.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN, some strategies include:\n",
    "\n",
    "\n",
    "    -Feature Selection or Dimensionality Reduction: Use techniques such as feature selection or dimensionality reduction methods (e.g., Principal Component Analysis, t-SNE) to reduce the number of dimensions and focus on the most informative features.\n",
    "\n",
    "\n",
    "    -Distance Metrics: Utilize distance metrics that are more suitable for high-dimensional spaces, such as distance measures specifically designed to handle sparsity or weighted distance functions.\n",
    "\n",
    "\n",
    "    -Data Preprocessing: Normalize or scale the data to address varying scales or ranges across different dimensions. This can help prevent certain features from dominating the distance calculations.\n",
    "\n",
    "\n",
    "    -Domain Knowledge: Incorporate domain knowledge to guide feature selection or engineering, identifying the most relevant features for the problem at hand.\n",
    "\n",
    "By addressing the challenges posed by the curse of dimensionality, it is possible to enhance the performance of KNN on high-dimensional data and improve its ability to capture meaningful patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a790949-e928-42bb-bee8-e2a4ef5b93e6",
   "metadata": {},
   "source": [
    "##  Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Handling missing values in K-Nearest Neighbors (KNN) algorithm requires imputation or handling of missing data points. Here are a few common approaches to deal with missing values in KNN:\n",
    "\n",
    "\n",
    "    1.Deletion: One straightforward approach is to simply delete data points that have missing values. However, this approach can lead to a significant loss of data, especially if the missing values are prevalent. It is typically used when the missing values are minimal or occur randomly without affecting the overall data distribution.\n",
    "\n",
    "\n",
    "    2.Mean/Median Imputation: In this approach, the missing values are replaced with the mean or median value of the respective feature. For each feature with missing values, you calculate the mean or median value from the available data and fill in the missing values with that value. This method assumes that the missing values are missing at random and that the mean or median is a reasonable estimate for the missing values.\n",
    "\n",
    "\n",
    "    3.Mode Imputation: Mode imputation is used for categorical variables. It involves replacing missing values with the most frequent category (mode) of the respective feature. This approach is applicable when dealing with categorical or nominal data.\n",
    "\n",
    "\n",
    "    4.KNN Imputation: KNN can also be used to impute missing values. In this approach, the KNN algorithm is applied to estimate the missing values based on the values of the nearest neighbors. For each missing value, the algorithm finds the K nearest neighbors based on the available features, and then the missing value is imputed by averaging or interpolating the values of the nearest neighbors. This method considers the relationship between the features and can provide better imputation results compared to simple mean or median imputation.\n",
    "\n",
    "\n",
    "    5.Model-Based Imputation: Another approach is to use machine learning models, such as regression or decision trees, to predict missing values based on the other available features. These models are trained on the instances with complete data and then used to predict the missing values. This method can capture more complex relationships in the data but requires additional computational resources and may introduce some bias.\n",
    "\n",
    "It is important to note that the choice of the imputation method depends on the nature of the data, the extent of missing values, and the underlying assumptions. It is recommended to carefully analyze the missing data pattern, consider the potential impact of imputation on the results, and evaluate the performance of different imputation techniques on the specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a321878-40b3-4383-8d23-6c9de07ef872",
   "metadata": {},
   "source": [
    "##  Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "The performance of the K-Nearest Neighbors (KNN) classifier and regressor can differ based on the nature of the problem and the type of data. Here is a comparison of their characteristics and recommendations for which type of problem each is better suited for:\n",
    "\n",
    "\n",
    "    1.KNN Classifier:\n",
    "\n",
    "-Prediction Task: The KNN classifier is used for classification tasks, where the goal is to assign data points to predefined classes or categories.\n",
    "\n",
    "-Output: The KNN classifier provides categorical labels or class assignments as the output.\n",
    "\n",
    "-Evaluation Metrics: Performance metrics such as accuracy, precision, recall, F1 score, and confusion matrix are commonly used to evaluate the performance of a KNN classifier.\n",
    "\n",
    "-Problem Types: The KNN classifier is suitable for problems where the target variable is discrete or categorical, and the goal is to classify instances into different classes or categories.\n",
    "\n",
    "-Example Applications: KNN classifier can be applied to various classification tasks, such as image classification, text classification, sentiment analysis, and spam detection.\n",
    "\n",
    "\n",
    "\n",
    "     2.KNN Regressor:\n",
    "        \n",
    "\n",
    "-Prediction Task: The KNN regressor is used for regression tasks, where the goal is to predict a continuous or numeric value for a given input data point.\n",
    "\n",
    "-Output: The KNN regressor provides continuous or numeric predictions as the output.\n",
    "\n",
    "-Evaluation Metrics: Evaluation metrics for KNN regressor typically include mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and R-squared (R2) score.\n",
    "\n",
    "-Problem Types: The KNN regressor is suitable for problems where the target variable is continuous or numeric, and the goal is to predict a specific value or estimate a quantity.\n",
    "\n",
    "-Example Applications: KNN regressor can be used in various regression tasks, such as predicting housing prices, stock market analysis, demand forecasting, and medical data analysis.\n",
    "\n",
    "In terms of selecting which one is better for a specific problem, consider the nature of the target variable and the problem requirements:\n",
    "\n",
    "\n",
    "    -For problems where the target variable is categorical and the goal is to classify instances into classes or categories, the KNN classifier is more appropriate.\n",
    "\n",
    "\n",
    "    -For problems where the target variable is continuous or numeric and the goal is to predict specific values or estimate quantities, the KNN regressor is more suitable.\n",
    "\n",
    "It is essential to carefully understand the problem requirements, evaluate the characteristics of the data, and consider the specific evaluation metrics to choose between the KNN classifier and regressor effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf79ecc-0100-4070-be9f-68b00e9e86cb",
   "metadata": {},
   "source": [
    "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm has its own strengths and weaknesses for both classification and regression tasks. Understanding these aspects can help in leveraging the strengths and mitigating the weaknesses. Here are the strengths and weaknesses of KNN:\n",
    "\n",
    "\n",
    "    --Strengths of KNN:\n",
    "\n",
    "1.Simplicity and Intuitiveness: KNN is a simple and intuitive algorithm that is easy to understand and implement. It does not make strong assumptions about the data distribution or underlying relationships, making it versatile.\n",
    "\n",
    "2.Non-Parametric Approach: KNN is a non-parametric algorithm, which means it doesn't assume a specific functional form for the data. It can be applied to various types of data, including linear and non-linear relationships.\n",
    "\n",
    "3.Flexibility: KNN can handle both classification and regression tasks. It can adapt to different types of data and is not limited to specific types of features or relationships.\n",
    "\n",
    "4.Locality-Based Learning: KNN relies on the local patterns in the data. It can capture complex decision boundaries and can be effective when the decision boundaries are irregular or when the classes or targets have overlapping regions.\n",
    "\n",
    "\n",
    "    --Weaknesses of KNN:\n",
    "\n",
    "1.Computational Complexity: The main drawback of KNN is its computational complexity. As the dataset grows, the time and memory required for searching nearest neighbors increase significantly. This can make KNN computationally expensive, especially for large datasets.\n",
    "\n",
    "2.Sensitivity to Feature Scaling: KNN calculates distances between data points, so the scale of the features can affect the algorithm's performance. Features with larger scales can dominate the distance calculation, leading to biased results. It is important to normalize or scale the features before applying KNN to avoid this issue.\n",
    "\n",
    "3.Curse of Dimensionality: KNN's performance deteriorates in high-dimensional spaces due to the curse of dimensionality. As the number of features increases, the data becomes more sparse, and the notion of proximity becomes less reliable. The algorithm can struggle to find meaningful neighbors and produce accurate predictions.\n",
    "\n",
    "4.Imbalanced Data Handling: KNN can be biased towards the majority class in imbalanced datasets. Since it relies on voting or averaging, the majority class tends to have more influence on the predictions. This can result in lower performance for minority classes or outliers.\n",
    "\n",
    "\n",
    "To address these weaknesses and enhance the performance of KNN:\n",
    "    \n",
    "\n",
    "-Feature Scaling: Normalize or scale the features to ensure that no single feature dominates the distance calculations.\n",
    "\n",
    "-Dimensionality Reduction: Apply dimensionality reduction techniques such as Principal Component Analysis (PCA) or feature selection to reduce the number of features and address the curse of dimensionality.\n",
    "\n",
    "-Efficient Data Structures: Utilize efficient data structures, such as KD-trees or ball trees, to speed up the nearest neighbor search process and improve computational efficiency.\n",
    "\n",
    "-Distance Metrics: Experiment with different distance metrics to find the most appropriate one for your data. For example, using a weighted distance metric or incorporating domain knowledge can enhance the performance.\n",
    "\n",
    "-Handling Imbalanced Data: Use techniques such as oversampling, undersampling, or algorithmic modifications (e.g., adjusting class weights) to handle imbalanced datasets and mitigate the bias towards the majority class.\n",
    "\n",
    "By considering these strategies, it is possible to address the weaknesses of KNN and improve its performance for classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01556355-6bb1-450b-a396-7970b8a92f83",
   "metadata": {},
   "source": [
    "##  Q9. Difference between Euclidean distance and Manhattan distance in KNN:\n",
    "\n",
    "Euclidean distance and Manhattan distance are both distance metrics used in the K-Nearest Neighbors (KNN) algorithm to measure the similarity or dissimilarity between data points. The main differences between these two distance metrics are:\n",
    "\n",
    "\n",
    "    1.Calculation Method:\n",
    "\n",
    "Euclidean Distance: It is calculated as the straight-line or Euclidean distance between two points in a Cartesian coordinate system. In a 2-dimensional space, the Euclidean distance between points (x1, y1) and (x2, y2) is calculated as sqrt((x2-x1)^2 + (y2-y1)^2). It extends to higher dimensions in a similar manner.\n",
    "Manhattan Distance: It is calculated as the sum of the absolute differences between the coordinates of two points. In a 2-dimensional space, the Manhattan distance between points (x1, y1) and (x2, y2) is calculated as |x2-x1| + |y2-y1|.\n",
    "\n",
    "\n",
    "    2.Geometric Interpretation:\n",
    "\n",
    "Euclidean Distance: Euclidean distance represents the shortest straight-line distance between two points. It is the direct distance between the points, considering their relative positions.\n",
    "Manhattan Distance: Manhattan distance represents the distance between two points when only horizontal and vertical movements are allowed. It is like measuring the distance while navigating through a grid-like city block structure.\n",
    "\n",
    "    3.Sensitivity to Feature Scales:\n",
    "\n",
    "Euclidean Distance: Euclidean distance is sensitive to the scale or magnitude of the features. If the features have different scales or units, the feature with a larger scale will dominate the distance calculation. Therefore, feature scaling is recommended when using Euclidean distance in KNN.\n",
    "Manhattan Distance: Manhattan distance is not as sensitive to feature scales as Euclidean distance. It considers the absolute differences between the coordinates, making it more robust to differences in feature scales.\n",
    "The choice between Euclidean distance and Manhattan distance depends on the characteristics of the data and the specific problem. Euclidean distance is commonly used when the dataset has continuous and normally distributed features. Manhattan distance, on the other hand, is often used when dealing with features that are measured on different scales or when working with categorical or ordinal data.\n",
    "\n",
    "##  Q10. Role of feature scaling in KNN:\n",
    "\n",
    "Feature scaling plays an important role in K-Nearest Neighbors (KNN) algorithm due to its reliance on distance calculations. Here's the role of feature scaling in KNN:\n",
    "\n",
    "1.Balancing Feature Influence: In KNN, distance-based metrics are used to measure the similarity between data points. If the features have different scales or units, those with larger scales will have a larger impact on the distance calculation. Feature scaling helps balance the influence of features by bringing them to a similar scale. It ensures that no single feature dominates the distance calculations.\n",
    "\n",
    "2.Improving Distance Accuracy: Scaling the features ensures that the distances calculated are more accurate and meaningful. If features are not scaled, the distances computed by KNN may not accurately reflect the true dissimilarity between data points. Feature scaling helps to align the feature values on a similar scale, allowing for more reliable distance calculations.\n",
    "\n",
    "3.Avoiding Biased Results: When features have significantly different scales, KNN can be biased towards features with larger scales. This can lead to suboptimal predictions or misinterpretations. Feature scaling helps to prevent such bias and ensures that the algorithm considers all features fairly.\n",
    "\n",
    "4.Faster Computation: Feature scaling can also have a positive impact on the computational efficiency of KNN. By scaling the features, the nearest neighbor search process can be performed more efficiently, as the distances between points become more comparable.\n",
    "\n",
    "Common techniques for feature scaling include normalization (such as Min-Max scaling) and standardization (such as z-score scaling). These methods adjust the features to specific ranges or distributions, enabling better performance and more meaningful distance calculations in KNN.\n",
    "\n",
    "In summary, feature scaling is crucial in KNN to ensure fair and accurate distance calculations, avoid bias, and improve the overall performance and interpretability of the algorithm.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e290392-9bbc-45da-aafd-02afd74a749e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
